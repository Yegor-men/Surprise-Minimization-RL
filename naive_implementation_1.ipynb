{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision.models.segmentation import LRASPP_MobileNet_V3_Large_Weights\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ],
   "id": "4f93562af163083d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class FixedSizeQueue:\n",
    "    def __init__(self, max_length):\n",
    "        self.queue = deque([0] * max_length, maxlen=max_length)\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        self.queue.append(item)\n",
    "\n",
    "    def get_average(self):\n",
    "        return sum(self.queue) / len(self.queue)\n",
    "\n",
    "    def get_last(self):\n",
    "        return self.queue[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(list(self.queue))"
   ],
   "id": "2c0bae81a5571213",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class Creature:\n",
    "    def __init__(self, tps):\n",
    "        self.ticks_per_second = tps\n",
    "        self.time_resolution = 1 / self.ticks_per_second\n",
    "        self.mass = 0.1\n",
    "        self.gravity = 9.81\n",
    "        self.friction_coefficient = 0.1\n",
    "        self.f_friction = self.friction_coefficient * self.mass * self.gravity\n",
    "        self.battery = 100\n",
    "        self.velocity_history = FixedSizeQueue(self.ticks_per_second * 1)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.battery, self.velocity_history.get_last()\n",
    "\n",
    "    def battery_discharge(self, x: float):\n",
    "        \"\"\"Given a certain requested force, by how much to lower the battery\"\"\"\n",
    "        return ((x / 5) ** 2) * self.time_resolution\n",
    "\n",
    "    def apply_force(self, force: float) -> None:\n",
    "        self.battery = min(self.battery + (1 * self.time_resolution), 100)\n",
    "        battery_to_discharge = self.battery_discharge(abs(force))\n",
    "        if battery_to_discharge <= self.battery:\n",
    "            v_old = self.velocity_history.get_last()\n",
    "            friction = -self.f_friction if (v_old > 0) else (self.f_friction if v_old < 0 else 0)\n",
    "            f_net = force + friction\n",
    "            \"\"\" friction acts opposite current velocity \"\"\"\n",
    "            friction_deceleration = True if abs(force) < self.f_friction else False\n",
    "            \"\"\" is the object decelerating due to friction and not applied force? if so then it naturally slows to a halt as the force is weaker then friction, it will not move the other way \"\"\"\n",
    "            delta_v = (f_net * self.time_resolution) / self.mass\n",
    "            v_new = v_old + delta_v\n",
    "            v_new = 0 if (friction_deceleration and v_new * v_old < 0) else v_new\n",
    "            \"\"\" checks if v_old to v_new has made a switch from pos to neg or vice versa; if decelerating due to friction, then v_new must be 0 as stated above \"\"\"\n",
    "            self.velocity_history.enqueue(v_new)\n",
    "            self.battery -= battery_to_discharge * self.time_resolution\n",
    "        else:\n",
    "            self.velocity_history.enqueue(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, latent_size: int, num_inputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.prev_latent = torch.zeros(latent_size)\n",
    "        self.prev_force = torch.zeros(1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "        self.mean_history = []\n",
    "        self.std_history = []\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(latent_size + num_inputs, latent_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latent_size * 2, latent_size),\n",
    "        )\n",
    "\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(latent_size, latent_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latent_size * 2, 1)\n",
    "        )\n",
    "\n",
    "        self.std = nn.Sequential(\n",
    "            nn.Linear(latent_size, latent_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latent_size * 2, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(latent_size + 1, latent_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(latent_size * 2, latent_size),\n",
    "        )\n",
    "\n",
    "    def update_hidden_states(self, force, latent):\n",
    "        self.prev_force = force\n",
    "        self.prev_latent = latent\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.prev_force = self.prev_force.detach()\n",
    "        self.prev_latent = self.prev_latent.detach()\n",
    "\n",
    "        tmp_1 = torch.cat([inputs, self.prev_latent], dim=0)\n",
    "        curr_latent = self.prev_latent + self.encoder(tmp_1)\n",
    "        curr_latent = self.layer_norm(curr_latent)\n",
    "\n",
    "        y1_mean = self.mean(curr_latent)\n",
    "        y1_std = self.std(curr_latent) + 0.001\n",
    "        self.mean_history.append(y1_mean.item())\n",
    "        self.std_history.append(y1_std.item())\n",
    "\n",
    "        tmp_2 = torch.cat([self.prev_latent, self.prev_force], dim=0)\n",
    "        predicted_latent = self.prev_latent + self.predictor(tmp_2)\n",
    "        predicted_latent = self.layer_norm(predicted_latent)\n",
    "\n",
    "        dist = torch.distributions.Normal(y1_mean, y1_std)\n",
    "        y1 = dist.rsample()\n",
    "        z = torch.abs(y1 - y1_mean) / y1_std\n",
    "        standard_normal = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n",
    "        probability = (1 - standard_normal.cdf(z)) * 2\n",
    "\n",
    "        return curr_latent, predicted_latent, y1, y1_std, probability"
   ],
   "id": "2975b64e778ca0a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class RewardFunction(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, real_latent, predicted_latent, velocity_reward, probability):\n",
    "        euclid_dist = torch.cdist(real_latent, predicted_latent)\n",
    "        surprise_factor = -euclid_dist\n",
    "        weighted_velocity_reward = velocity_reward\n",
    "        composite_reward = weighted_velocity_reward + surprise_factor\n",
    "        loss = torch.exp(-composite_reward)\n",
    "        return loss, weighted_velocity_reward, surprise_factor, euclid_dist"
   ],
   "id": "91fc0d2d8b110717",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SECONDS = 100\n",
    "LR_PER_SECOND = 1\n",
    "TICKS_PER_SECOND = 100\n",
    "\n",
    "robot = Creature(\n",
    "    tps=TICKS_PER_SECOND\n",
    ")\n",
    "model = Model(\n",
    "    latent_size=64,\n",
    "    num_inputs=2,\n",
    ")\n",
    "\n",
    "LEARNING_RATE = LR_PER_SECOND * (1 / TICKS_PER_SECOND)\n",
    "TIMESTEPS = SECONDS * TICKS_PER_SECOND\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "reward_fn = RewardFunction()\n",
    "\n",
    "velocity_hist = []\n",
    "avg_velocity_hist = []\n",
    "battery_hist = []\n",
    "loss_history = []\n",
    "surprise_factor_history = []\n",
    "v_reward_history = []\n",
    "s_reward_history = []\n",
    "total_reward_history = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for timestep in range(TIMESTEPS):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    battery, v_last = robot.get_data()\n",
    "\n",
    "    data = torch.Tensor([battery / 100, v_last])\n",
    "\n",
    "    curr_lat, pred_lat, force, std, prob = model(data)\n",
    "\n",
    "    robot.apply_force(force.item())\n",
    "    v_curr = robot.velocity_history.get_last()\n",
    "    v_avg = robot.velocity_history.get_average()\n",
    "\n",
    "    velocity_hist.append(v_curr)\n",
    "    avg_velocity_hist.append(v_avg)\n",
    "    battery_hist.append(robot.battery)\n",
    "\n",
    "    loss, v_reward, s_reward, s_factor = reward_fn(\n",
    "        real_latent=curr_lat.unsqueeze(0),\n",
    "        predicted_latent=pred_lat.unsqueeze(0),\n",
    "        velocity_reward=torch.Tensor([v_curr]),\n",
    "        probability=prob\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "    surprise_factor_history.append(s_factor.item())\n",
    "    v_reward_history.append(v_reward.item())\n",
    "    s_reward_history.append(s_reward.item())\n",
    "    total_reward_history.append(v_reward.item() + s_reward.item())\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    model.update_hidden_states(\n",
    "        force=force,\n",
    "        latent=curr_lat,\n",
    "    )\n",
    "\n",
    "    if (timestep + 1) % 100 == 0:\n",
    "        print(f\"{(timestep + 1) / TICKS_PER_SECOND:.3f}s - {((timestep + 1) / TIMESTEPS) * 100:.2f}%\", end=\" \")\n",
    "        print(f\"| V {v_curr:.5f}\", end=\" \")\n",
    "        print(f\"| AV {v_avg:.5f}\", end=\" \")\n",
    "        print(f\"| SF {s_factor.item():.5f}\", end=\" \")\n",
    "        print(f\"| VR {v_reward.item():.3f}\", end=\" \")\n",
    "        print(f\"| SR {s_reward.item():.3f}\", end=\" \")\n",
    "        print(f\"| Pr {prob.item():.3f}\", end=\" \")\n",
    "        print(f\"| L {loss.item():.8f}\", end=\"\\n\")"
   ],
   "id": "89e8b69e8994856e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 16))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].plot(velocity_hist)\n",
    "axes[0].set_title('Velocity')\n",
    "\n",
    "axes[1].plot(avg_velocity_hist)\n",
    "axes[1].set_title('Average Velocity')\n",
    "\n",
    "axes[2].plot(model.mean_history)\n",
    "axes[2].set_title('Mean')\n",
    "\n",
    "axes[3].plot(model.std_history)\n",
    "axes[3].set_title('Standard deviation')\n",
    "\n",
    "axes[4].plot(loss_history)\n",
    "axes[4].set_title('Loss (lower is better)')\n",
    "\n",
    "axes[5].plot(surprise_factor_history)\n",
    "axes[5].set_title('Surprise Factor')\n",
    "\n",
    "axes[6].plot(battery_hist)\n",
    "axes[6].set_title('Battery')\n",
    "\n",
    "axes[7].plot(v_reward_history, label=\"Velocity\")\n",
    "axes[7].plot(s_reward_history, label=\"S factor\")\n",
    "axes[7].plot(total_reward_history, label=\"Total\")\n",
    "axes[7].set_title('Reward')\n",
    "axes[7].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "da1eda028654ed84",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
